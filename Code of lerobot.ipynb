{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ade9180-e99e-4a80-9ff8-04fb3d1fb071",
   "metadata": {},
   "source": [
    "### Test of robot synchronization control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b743e-634e-48fd-bc85-b480cceea67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the project root path to ensure proper module import\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "project_root = \"/home/wjw/lerobot\"\n",
    "sys.path.insert(0, project_root)\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(\"Project root directory set to:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2daba3-7d6c-47ed-8569-908f8b7cb953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize robot and cameras (enhanced version)\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import numpy as np\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from lerobot.common.robot_devices.robots.configs import KochRobotConfig\n",
    "from lerobot.common.robot_devices.robots.manipulator import ManipulatorRobot\n",
    "from lerobot.common.robot_devices.cameras.configs import OpenCVCameraConfig\n",
    "\n",
    "# Configure the robot and attach cameras\n",
    "robot_cfg = KochRobotConfig()\n",
    "robot_cfg.cameras = {\n",
    "    \"e22s_side\": OpenCVCameraConfig(camera_index=0, fps=30, width=640, height=480),  # E22S side view\n",
    "    \"e12_top\": OpenCVCameraConfig(camera_index=4, fps=30, width=640, height=480),    # E12 top view\n",
    "}\n",
    "\n",
    "# Initialize the robot instance\n",
    "robot = ManipulatorRobot(robot_cfg)\n",
    "\n",
    "# Attempt connection\n",
    "print(\"Connecting to robot (initial calibration will start if required)...\")\n",
    "try:\n",
    "    robot.connect()\n",
    "    print(\"Robot connection established.\")\n",
    "\n",
    "    # Display calibration file path\n",
    "    print(\"Calibration files saved to:\", robot_cfg.calibration_dir)\n",
    "except Exception as e:\n",
    "    print(f\"Robot connection failed: {e}\")\n",
    "\n",
    "# Print camera information for verification\n",
    "print(\"Cameras initialized:\")\n",
    "for name, cam in robot_cfg.cameras.items():\n",
    "    print(f\" - {name}: /dev/video{cam.camera_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d6c43f-126e-430b-bd9e-7828d532271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test leader-follower synchronization\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "print(\"Starting leader-follower synchronization test for 30 seconds at 200Hz...\")\n",
    "seconds = 30\n",
    "frequency = 200\n",
    "interval = 1 / frequency\n",
    "\n",
    "for _ in tqdm.tqdm(range(seconds * frequency)):\n",
    "    leader_pos = robot.leader_arms[\"main\"].read(\"Present_Position\")\n",
    "    robot.follower_arms[\"main\"].write(\"Goal_Position\", leader_pos)\n",
    "    time.sleep(interval)\n",
    "\n",
    "print(\"Leader-follower synchronization test completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5dc493-8714-4ea8-8ff3-5b646f353a77",
   "metadata": {},
   "source": [
    "### Recording data for Section 6 - Joint-State-Based ACT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb70e2c-a1c9-4f16-b372-a54f3671d7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-precision data recording script (without image queue) - final version\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from lerobot.scripts.control_robot import busy_wait  # High-precision timing utility\n",
    "\n",
    "# Automatically assign the next available clip ID\n",
    "def get_next_clip_id(base_dir=\"recordings\"):\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    existing = sorted([d for d in os.listdir(base_dir) if d.startswith(\"clip_\")])\n",
    "    if not existing:\n",
    "        return 1\n",
    "    return int(existing[-1].split(\"_\")[-1]) + 1\n",
    "\n",
    "clip_id = get_next_clip_id()\n",
    "base_path = Path(\"recordings\") / f\"clip_{clip_id:05d}\"\n",
    "cam0_dir = base_path / \"images_cam0\"\n",
    "cam1_dir = base_path / \"images_cam1\"\n",
    "joint_dir = base_path / \"joint_states\"\n",
    "\n",
    "for d in [cam0_dir, cam1_dir, joint_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Access the camera devices\n",
    "cap0 = robot.cameras[\"e22s_side\"]\n",
    "cap1 = robot.cameras[\"e12_top\"]\n",
    "\n",
    "joint_log = []\n",
    "bad_frame_count = 0\n",
    "frame_id = 0\n",
    "target_fps = 20\n",
    "interval = 1 / target_fps\n",
    "\n",
    "# Initialize smoother\n",
    "smoothed_leader = None\n",
    "alpha = 0.3  # Smoothing factor (lower is smoother, higher is faster)\n",
    "\n",
    "print(f\"Recording clip_{clip_id:05d}. Press 'Q' to stop...\")\n",
    "\n",
    "while True:\n",
    "    start_time = time.perf_counter()  # High-resolution timer\n",
    "    try:\n",
    "        # Read leader joint positions\n",
    "        leader_pos = robot.leader_arms[\"main\"].read(\"Present_Position\")\n",
    "        if not isinstance(leader_pos, (list, np.ndarray)) or len(leader_pos) != 6:\n",
    "            raise ValueError(\"Invalid leader position format\")\n",
    "\n",
    "        leader_pos = np.array(leader_pos)\n",
    "\n",
    "        # Apply exponential smoothing\n",
    "        if smoothed_leader is None:\n",
    "            smoothed_leader = leader_pos.copy()\n",
    "        else:\n",
    "            smoothed_leader = alpha * leader_pos + (1 - alpha) * smoothed_leader\n",
    "\n",
    "        # Send smoothed position to follower\n",
    "        robot.follower_arms[\"main\"].write(\"Goal_Position\", smoothed_leader.tolist())\n",
    "\n",
    "        # Read follower joint positions\n",
    "        follower_pos = robot.follower_arms[\"main\"].read(\"Present_Position\")\n",
    "        follower_pos = np.array(follower_pos).tolist()\n",
    "        leader_pos = leader_pos.tolist()\n",
    "\n",
    "        # Capture images\n",
    "        frame0 = cap0.read()\n",
    "        frame1 = cap1.read()\n",
    "        if frame0 is None or frame1 is None:\n",
    "            print(\"Image read failed. Skipping frame.\")\n",
    "            bad_frame_count += 1\n",
    "            continue\n",
    "\n",
    "        # Save images\n",
    "        cv2.imwrite(str(cam0_dir / f\"frame_{frame_id:05d}.jpg\"), frame0)\n",
    "        cv2.imwrite(str(cam1_dir / f\"frame_{frame_id:05d}.jpg\"), frame1)\n",
    "\n",
    "        # Save joint data with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        joint_log.append([timestamp, leader_pos, follower_pos])\n",
    "\n",
    "        # Show camera previews\n",
    "        cv2.imshow(\"E22S\", frame0)\n",
    "        cv2.imshow(\"E12\", frame1)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            print(\"Recording stopped by user.\")\n",
    "            break\n",
    "\n",
    "        frame_id += 1\n",
    "\n",
    "        # Busy-wait to maintain constant FPS\n",
    "        dt = time.perf_counter() - start_time\n",
    "        busy_wait(interval - dt)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")\n",
    "        bad_frame_count += 1\n",
    "        continue\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save joint log as pickle file\n",
    "with open(joint_dir / \"joint_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(joint_log, f)\n",
    "\n",
    "print(f\"Recording clip_{clip_id:05d} completed. Valid frames: {len(joint_log)}, Dropped frames: {bad_frame_count}\")\n",
    "print(\"Saved to:\", cam0_dir)\n",
    "print(\"Saved to:\", cam1_dir)\n",
    "print(\"Saved to:\", joint_dir / \"joint_data.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f9d46-8217-4a12-9f1a-f96ed5e830d4",
   "metadata": {},
   "source": [
    "### Evaluation of recorded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3965648-4464-4e7a-949b-c9a2b29dd66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Root directory containing recorded clips (update path as needed)\n",
    "data_root = Path(\"/home/wjw/lerobot/results/recordings\")\n",
    "\n",
    "# Get all clip folders\n",
    "clip_folders = sorted([d for d in data_root.iterdir() if d.is_dir() and d.name.startswith(\"clip_\")])\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate through each clip and check if image and joint data frame counts match\n",
    "for clip_path in clip_folders:\n",
    "    clip_name = clip_path.name\n",
    "\n",
    "    cam0_dir = clip_path / \"images_cam0\"\n",
    "    cam1_dir = clip_path / \"images_cam1\"\n",
    "    joint_path = clip_path / \"joint_states\" / \"joint_data.pkl\"\n",
    "\n",
    "    # Count image frames\n",
    "    cam0_count = len(list(cam0_dir.glob(\"*.jpg\")))\n",
    "    cam1_count = len(list(cam1_dir.glob(\"*.jpg\")))\n",
    "\n",
    "    # Count joint data frames\n",
    "    try:\n",
    "        with open(joint_path, \"rb\") as f:\n",
    "            joint_data = pickle.load(f)\n",
    "        joint_count = len(joint_data)\n",
    "    except Exception as e:\n",
    "        joint_count = 0\n",
    "\n",
    "    # Check frame alignment\n",
    "    aligned = (cam0_count == cam1_count == joint_count)\n",
    "\n",
    "    results.append({\n",
    "        \"Clip\": clip_name,\n",
    "        \"cam0_frames\": cam0_count,\n",
    "        \"cam1_frames\": cam1_count,\n",
    "        \"joint_frames\": joint_count,\n",
    "        \"Aligned\": \"Yes\" if aligned else \"No\"\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "df_sync = pd.DataFrame(results)\n",
    "print(\"Frame synchronization check completed. Results:\")\n",
    "\n",
    "display(df_sync)\n",
    "\n",
    "# Export DataFrame to LaTeX table\n",
    "latex_table = df_sync.to_latex(index=False, caption=\"Frame Synchronization Summary\", label=\"tab:sync_summary\")\n",
    "\n",
    "# Save to specified output path\n",
    "output_path = Path(\"/home/wjw/lerobot/results/recordings/frame_sync_summary.tex\")\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(f\"LaTeX table saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4687d8-e90a-48c6-8298-4245c01c00aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Set root path and output directory\n",
    "data_root = Path(\"/home/wjw/lerobot/results/recordings\")\n",
    "clip_folders = sorted([d for d in data_root.iterdir() if d.is_dir() and d.name.startswith(\"clip_\")])\n",
    "save_dir = data_root / \"trajectory_plots\"\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Iterate over each clip and generate trajectory comparison plots\n",
    "for clip_path in clip_folders:\n",
    "    joint_file = clip_path / \"joint_states\" / \"joint_data.pkl\"\n",
    "    try:\n",
    "        with open(joint_file, \"rb\") as f:\n",
    "            joint_data = pickle.load(f)\n",
    "\n",
    "        leader_trajectories = np.array([entry[1] for entry in joint_data])\n",
    "        follower_trajectories = np.array([entry[2] for entry in joint_data])\n",
    "        time = np.arange(len(leader_trajectories))\n",
    "        num_joints = leader_trajectories.shape[1]\n",
    "\n",
    "        fig, axes = plt.subplots(num_joints, 1, figsize=(12, 2.5 * num_joints), sharex=True)\n",
    "        for j in range(num_joints):\n",
    "            axes[j].plot(time, leader_trajectories[:, j], label=\"Leader Arm\", linestyle='--')\n",
    "            axes[j].plot(time, follower_trajectories[:, j], label=\"Follower Arm\", alpha=0.8)\n",
    "            axes[j].set_ylabel(f\"Joint {j+1} (deg)\")\n",
    "            axes[j].legend(loc=\"upper right\")\n",
    "            axes[j].grid(True)\n",
    "\n",
    "        axes[-1].set_xlabel(\"Frame Index\")\n",
    "        fig.suptitle(f\"Trajectory Comparison for {clip_path.name}\", fontsize=16)\n",
    "        fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        fig.savefig(save_dir / f\"{clip_path.name}_trajectory.png\", dpi=200)\n",
    "        plt.close(fig)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {clip_path.name}: {e}\")\n",
    "\n",
    "print(f\"All trajectory plots saved to: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb803d-91f0-41e5-92c6-30cde9b0669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Set root directory for recorded data\n",
    "data_root = Path(\"/home/wjw/lerobot/results/recordings\")\n",
    "clip_folders = sorted([d for d in data_root.iterdir() if d.is_dir() and d.name.startswith(\"clip_\")])\n",
    "save_dir = data_root / \"error_analysis_plots\"\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "summary_list = []\n",
    "\n",
    "# Analyze tracking errors for each clip\n",
    "for clip_path in clip_folders:\n",
    "    joint_file = clip_path / \"joint_states\" / \"joint_data.pkl\"\n",
    "    try:\n",
    "        with open(joint_file, \"rb\") as f:\n",
    "            joint_data = pickle.load(f)\n",
    "\n",
    "        leader_trajectories = np.array([entry[1] for entry in joint_data])\n",
    "        follower_trajectories = np.array([entry[2] for entry in joint_data])\n",
    "\n",
    "        if leader_trajectories.shape != follower_trajectories.shape:\n",
    "            print(f\"Skipping {clip_path.name}: shape mismatch\")\n",
    "            continue\n",
    "\n",
    "        errors = leader_trajectories - follower_trajectories\n",
    "        num_joints = errors.shape[1]\n",
    "        time = np.arange(errors.shape[0])\n",
    "\n",
    "        # Plot error curves for each joint\n",
    "        fig, axes = plt.subplots(num_joints, 1, figsize=(12, 2.5 * num_joints), sharex=True)\n",
    "        for j in range(num_joints):\n",
    "            axes[j].plot(time, errors[:, j], label=\"Error\", color=\"red\")\n",
    "            axes[j].axhline(0, color='black', linestyle='--', linewidth=0.8)\n",
    "            axes[j].set_ylabel(f\"Joint {j+1} Error (rad)\")\n",
    "            axes[j].grid(True)\n",
    "        axes[-1].set_xlabel(\"Frame Index\")\n",
    "        fig.suptitle(f\"Joint Angle Error: {clip_path.name}\", fontsize=16)\n",
    "        fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        fig.savefig(save_dir / f\"{clip_path.name}_error.png\", dpi=200)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Append clip-level error statistics\n",
    "        summary_list.append({\n",
    "            \"Clip\": clip_path.name,\n",
    "            **{f\"Mean_J{i+1}\": np.mean(errors[:, i]) for i in range(num_joints)},\n",
    "            **{f\"Max_J{i+1}\": np.max(np.abs(errors[:, i])) for i in range(num_joints)},\n",
    "            **{f\"Std_J{i+1}\": np.std(errors[:, i]) for i in range(num_joints)}\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {clip_path.name}: {e}\")\n",
    "\n",
    "# Generate error summary table\n",
    "error_df_all = pd.DataFrame(summary_list)\n",
    "csv_path = data_root / \"error_summary_all.csv\"\n",
    "error_df_all.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"All error plots saved to: {save_dir}\")\n",
    "print(f\"Error summary CSV saved to: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770eab4-c11a-4ea1-8ede-285050ae7854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Set root directory for recorded clips\n",
    "data_root = Path(\"/home/wjw/lerobot/results/recordings\")\n",
    "clip_folders = sorted([d for d in data_root.iterdir() if d.is_dir() and d.name.startswith(\"clip_\")])\n",
    "save_dir = data_root / \"velocity_plots\"\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Iterate through each clip and compute joint velocities\n",
    "for clip_path in clip_folders:\n",
    "    joint_file = clip_path / \"joint_states\" / \"joint_data.pkl\"\n",
    "\n",
    "    try:\n",
    "        with open(joint_file, \"rb\") as f:\n",
    "            joint_data = pickle.load(f)\n",
    "\n",
    "        # Extract leader and follower trajectories\n",
    "        leader_trajectories = np.array([entry[1] for entry in joint_data])\n",
    "        follower_trajectories = np.array([entry[2] for entry in joint_data])\n",
    "\n",
    "        if leader_trajectories.shape != follower_trajectories.shape:\n",
    "            print(f\"Skipping {clip_path.name} due to shape mismatch.\")\n",
    "            continue\n",
    "\n",
    "        # Compute angular velocities (frame-wise differences)\n",
    "        leader_velocity = np.diff(leader_trajectories, axis=0)\n",
    "        follower_velocity = np.diff(follower_trajectories, axis=0)\n",
    "        time = np.arange(len(leader_velocity))\n",
    "        num_joints = leader_velocity.shape[1]\n",
    "\n",
    "        # Plot velocity curves for each joint\n",
    "        fig, axes = plt.subplots(num_joints, 1, figsize=(12, 2.5 * num_joints), sharex=True)\n",
    "        for j in range(num_joints):\n",
    "            axes[j].plot(time, leader_velocity[:, j], label=\"Leader Velocity\", linestyle=\"--\", color=\"blue\")\n",
    "            axes[j].plot(time, follower_velocity[:, j], label=\"Follower Velocity\", linestyle=\"-\", color=\"orange\")\n",
    "            axes[j].set_ylabel(f\"Joint {j+1} Velocity (rad/frame)\")\n",
    "            axes[j].legend(loc=\"upper right\")\n",
    "            axes[j].grid(True)\n",
    "\n",
    "        axes[-1].set_xlabel(\"Frame Index\")\n",
    "        fig.suptitle(f\"Joint Velocity Analysis: {clip_path.name}\", fontsize=16)\n",
    "        fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "        fig.savefig(save_dir / f\"{clip_path.name}_velocity.png\", dpi=200)\n",
    "        plt.close(fig)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {clip_path.name}: {e}\")\n",
    "\n",
    "print(f\"All joint velocity plots have been saved to: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bcc546-85f8-48aa-89d7-8beb5117caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Set root directory\n",
    "data_root = Path(\"/home/wjw/lerobot/results/recordings\")\n",
    "clip_folders = sorted([d for d in data_root.iterdir() if d.is_dir() and d.name.startswith(\"clip_\")])\n",
    "save_dir = data_root / \"mse_plots\"\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "summary = []\n",
    "\n",
    "# Compute MSE for each clip\n",
    "for clip_path in clip_folders:\n",
    "    joint_file = clip_path / \"joint_states\" / \"joint_data.pkl\"\n",
    "\n",
    "    try:\n",
    "        with open(joint_file, \"rb\") as f:\n",
    "            joint_data = pickle.load(f)\n",
    "\n",
    "        leader = np.array([entry[1] for entry in joint_data])\n",
    "        follower = np.array([entry[2] for entry in joint_data])\n",
    "\n",
    "        if leader.shape != follower.shape:\n",
    "            print(f\"Skipping {clip_path.name} due to shape mismatch.\")\n",
    "            continue\n",
    "\n",
    "        # Compute per-frame MSE (mean squared error over joints)\n",
    "        mse_per_frame = np.mean((leader - follower) ** 2, axis=1)\n",
    "        overall_mse = np.mean(mse_per_frame)\n",
    "\n",
    "        # Plot MSE over time\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(mse_per_frame, label=\"MSE (radÂ²)\", color='red')\n",
    "        plt.xlabel(\"Frame Index\")\n",
    "        plt.ylabel(\"MSE\")\n",
    "        plt.title(f\"MSE Over Time: {clip_path.name}\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_dir / f\"{clip_path.name}_mse_curve.png\", dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "        summary.append({\n",
    "            \"Clip\": clip_path.name,\n",
    "            \"Total Frames\": len(mse_per_frame),\n",
    "            \"Overall MSE (rad^2)\": overall_mse\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {clip_path.name}: {e}\")\n",
    "\n",
    "# Export summary as CSV\n",
    "df_mse = pd.DataFrame(summary)\n",
    "csv_path = data_root / \"mse_summary.csv\"\n",
    "df_mse.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"All MSE plots saved to: {save_dir}\")\n",
    "print(f\"MSE summary table saved to: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663f190-795b-4c8b-a539-64ff0207e9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Set root directory and iterate over all clip folders\n",
    "recording_root = Path(\"/home/wjw/lerobot/results/recordings\")\n",
    "clip_dirs = sorted([p for p in recording_root.glob(\"clip_*\") if p.is_dir()])\n",
    "\n",
    "for clip_path in clip_dirs:\n",
    "    joint_path = clip_path / \"joint_states/joint_data.pkl\"\n",
    "    save_path = clip_path / \"error_heatmap.png\"\n",
    "\n",
    "    if not joint_path.exists():\n",
    "        print(f\"Skipping {clip_path.name}: joint_data.pkl not found.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(joint_path, \"rb\") as f:\n",
    "            joint_data = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {clip_path.name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Extract leader and follower trajectories\n",
    "    leader = np.array([entry[1] for entry in joint_data])\n",
    "    follower = np.array([entry[2] for entry in joint_data])\n",
    "    errors = np.abs(leader - follower).T  # Shape: (6 joints) x (frames)\n",
    "\n",
    "    # Plot static heatmap of joint errors\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    im = ax.imshow(errors, aspect='auto', cmap='turbo', interpolation='nearest')\n",
    "    cbar = fig.colorbar(im)\n",
    "    cbar.set_label(\"Joint Error (rad)\")\n",
    "\n",
    "    ax.set_title(f\"{clip_path.name} - Joint Error Heatmap\")\n",
    "    ax.set_yticks(range(6))\n",
    "    ax.set_yticklabels([f\"J{i+1}\" for i in range(6)])\n",
    "    ax.set_xlabel(\"Frame Index\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved static error heatmap: {save_path.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643fb497-b79f-4dda-8812-b0cc631be618",
   "metadata": {},
   "source": [
    "### Section 6 Joint-state-based ACT training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c506dee-f3c7-4b87-8ce3-610dbc788fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initialize robot and cameras (enhanced version)\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import numpy as np\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from lerobot.common.robot_devices.robots.configs import KochRobotConfig\n",
    "from lerobot.common.robot_devices.robots.manipulator import ManipulatorRobot\n",
    "from lerobot.common.robot_devices.cameras.configs import OpenCVCameraConfig\n",
    "\n",
    "# Configure the robot and attached cameras\n",
    "robot_cfg = KochRobotConfig()\n",
    "robot_cfg.cameras = {\n",
    "    \"e22s_side\": OpenCVCameraConfig(camera_index=0, fps=30, width=640, height=480),  # E22S side view\n",
    "    \"e12_top\": OpenCVCameraConfig(camera_index=4, fps=30, width=640, height=480),    # E12 top view\n",
    "}\n",
    "\n",
    "# Initialize the robot instance\n",
    "robot = ManipulatorRobot(robot_cfg)\n",
    "\n",
    "# Attempt to connect to the robot\n",
    "print(\"Connecting to the robot (initial calibration may start automatically)...\")\n",
    "try:\n",
    "    robot.connect()\n",
    "    print(\"Robot connection established.\")\n",
    "\n",
    "    # Display calibration file path (used to verify calibration process)\n",
    "    print(\"Calibration files saved to:\", robot_cfg.calibration_dir)\n",
    "except Exception as e:\n",
    "    print(f\"Robot connection failed: {e}\")\n",
    "\n",
    "# Print camera information for verification\n",
    "print(\"Cameras successfully initialized:\")\n",
    "for name, cam in robot_cfg.cameras.items():\n",
    "    print(f\" - {name}: /dev/video{cam.camera_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8faf2a-fc03-46a1-8703-ebb4aab13aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import required modules\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38088d0-8440-4e67-ba9d-b29ab738118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Set data path and load state-action data from all clips\n",
    "data_root = Path(\"/home/wjw/lerobot/results/recordings\")  # Directory containing 40 multi-position clips\n",
    "data_files = sorted(data_root.glob(\"clip_*/joint_states/state_action.npy\"))\n",
    "\n",
    "print(f\"Loaded {len(data_files)} state-action records.\")\n",
    "\n",
    "states, actions = [], []\n",
    "for f in data_files:\n",
    "    data = np.load(f, allow_pickle=True)\n",
    "    states.append(data[:, :6])   # First 6 columns: state\n",
    "    actions.append(data[:, 6:])  # Last 6 columns: action\n",
    "\n",
    "states = np.concatenate(states, axis=0)\n",
    "actions = np.concatenate(actions, axis=0)\n",
    "\n",
    "print(f\"Data merged successfully. State shape: {states.shape}, Action shape: {actions.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a66271e-5ce0-4d5f-92e4-c44a3a31a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Construct chunk sequences with context window\n",
    "chunk_size = 10  # Use 10 consecutive frames to predict the action of the 11th frame\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(states) - chunk_size):\n",
    "    X.append(states[i:i + chunk_size])\n",
    "    y.append(actions[i + chunk_size - 1])  # Predict the action of the last frame in the chunk\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "print(f\"Number of training samples: {len(X_train)}, validation samples: {len(X_val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1944cf-d3b7-4c03-a944-a73626fa8d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Build chunked sequences with a context window\n",
    "chunk_size = 10  # Use 10 input frames to predict the action of the 11th frame\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(states) - chunk_size):\n",
    "    X.append(states[i:i + chunk_size])\n",
    "    y.append(actions[i + chunk_size - 1])  # Target is the action of the last frame in the chunk\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18285960-1fbe-4696-901d-0baeb0f5f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Define dataset wrapper and DataLoader\n",
    "class ACTDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(ACTDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(ACTDataset(X_val, y_val), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1422619-1932-4162-9434-cc5e86b83807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Define the ACT Transformer model\n",
    "class ACTModel(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=256, num_heads=4, num_layers=3, out_dim=6):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)        # Shape: (batch, sequence length, hidden size)\n",
    "        x = self.encoder(x)           # Same shape\n",
    "        return self.output(x[:, -1])  # Use only the last timestep for action prediction\n",
    "\n",
    "model = ACTModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b119a-7736-4d89-af1e-ed23596bdf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Train the model with loss tracking and early stopping\n",
    "\n",
    "epochs = 200\n",
    "early_stop_patience = 20  # Stop training if no improvement on validation loss for 20 consecutive epochs\n",
    "best_val_loss = float('inf')\n",
    "no_improve_count = 0\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            val_loss += criterion(model(xb), yb).item()\n",
    "\n",
    "    train_losses.append(total_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {total_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve_count = 0\n",
    "        # Optionally save the best model\n",
    "        torch.save(model.state_dict(), \"models/best_model_auto_saved.pth\")\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "        if no_improve_count >= early_stop_patience:\n",
    "            print(f\"Early stopping triggered: no improvement for {early_stop_patience} consecutive epochs.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848abe8d-24d0-4257-a613-190a53d628dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.1: Plot training and validation loss curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8605974-5e00-4fa9-92ac-d476962e0830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Save the trained model\n",
    "save_path = Path(\"models/act_model_multi_position.pth\")\n",
    "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5d9e0-a5ff-4c23-a4e3-887e53e51023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Load the trained model and run inference on a validation sample\n",
    "model = ACTModel()\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.eval()\n",
    "\n",
    "# Select a random validation sample\n",
    "idx = np.random.randint(len(X_val))\n",
    "sample_input = torch.tensor(X_val[idx:idx + 1], dtype=torch.float32)\n",
    "ground_truth = y_val[idx]\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    pred_action = model(sample_input).squeeze().numpy()\n",
    "\n",
    "print(\"Ground Truth Action:     \", np.round(ground_truth, 2))\n",
    "print(\"Predicted Action Output: \", np.round(pred_action, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d2df6-1ec8-4898-a9a0-78675b0c0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Load the trained ACT model for deployment on LeRobot\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Redefine the ACT Transformer model architecture (must match training configuration)\n",
    "class ACTModel(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=256, num_heads=4, num_layers=3, out_dim=6):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.encoder(x)\n",
    "        return self.output(x[:, -1])  # Use only the final timestep for prediction\n",
    "\n",
    "# Load trained model weights\n",
    "model = ACTModel()\n",
    "model.load_state_dict(torch.load(\"models/act_model_multi_position.pth\"))\n",
    "model.eval()\n",
    "\n",
    "print(\"Model successfully loaded. Ready for deployment on LeRobot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74e3b5-f219-4fad-813f-a7ca4c2b6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 (Enhanced): Execute model-based control and record trajectory error\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load reference state-action sequence for deployment testing\n",
    "clip_dir = \"recordings/clip_00020/joint_states\"  # Change to any test clip as needed\n",
    "state_action = np.load(f\"{clip_dir}/state_action.npy\", allow_pickle=True)\n",
    "\n",
    "states = state_action[:, :6]\n",
    "true_actions = state_action[:, 6:]\n",
    "\n",
    "context_len = 10\n",
    "input_seq = []\n",
    "\n",
    "# Initialize storage for predicted and ground truth actions\n",
    "predicted_actions = []\n",
    "ground_truth_actions = []\n",
    "\n",
    "print(\"Deploying model to control the robot and record trajectory errors...\")\n",
    "\n",
    "for i in range(len(states)):\n",
    "    input_seq.append(states[i])\n",
    "\n",
    "    if len(input_seq) < context_len:\n",
    "        continue\n",
    "    elif len(input_seq) > context_len:\n",
    "        input_seq.pop(0)\n",
    "\n",
    "    input_tensor = torch.tensor([input_seq], dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_action = model(input_tensor).squeeze().numpy()\n",
    "\n",
    "    # Send predicted action to robot\n",
    "    robot.follower_arms[\"main\"].write(\"Goal_Position\", pred_action.tolist())\n",
    "    time.sleep(0.05)\n",
    "\n",
    "    # Record both predicted and ground truth actions\n",
    "    predicted_actions.append(pred_action)\n",
    "    ground_truth_actions.append(true_actions[i])\n",
    "\n",
    "print(\"Model control execution completed. Plotting trajectory comparison...\")\n",
    "\n",
    "# === Plotting section ===\n",
    "predicted_actions = np.array(predicted_actions)\n",
    "ground_truth_actions = np.array(ground_truth_actions)\n",
    "\n",
    "joint_labels = [f\"Joint {i+1}\" for i in range(6)]\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "for j in range(6):\n",
    "    plt.subplot(3, 2, j + 1)\n",
    "    plt.plot(ground_truth_actions[:, j], label='Ground Truth')\n",
    "    plt.plot(predicted_actions[:, j], label='Predicted', linestyle='--')\n",
    "    plt.title(f\"{joint_labels[j]} Trajectory\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Angle (deg)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/trajectory_compare_clip_00020.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Trajectory comparison plot saved to: results/trajectory_compare_clip_00020.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03ee73-5adc-4f9a-8b3a-0633afa9ff79",
   "metadata": {},
   "source": [
    "### Evaluation of Section6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48455b39-358d-4887-83d3-c3d1463ad09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Batch process all clips and save ground truth vs. predicted trajectories as CSV\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load trained model (must match architecture)\n",
    "model_path = \"models/act_model_multi_position.pth\"\n",
    "model = ACTModel()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "context_len = 10\n",
    "recordings_dir = Path(\"recordings\")\n",
    "output_dir = Path(\"results/trajectories\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Scan all available clips\n",
    "clip_files = sorted(recordings_dir.glob(\"clip_*/joint_states/state_action.npy\"))\n",
    "print(f\"Found {len(clip_files)} clips. Starting batch processing...\")\n",
    "\n",
    "for state_action_path in clip_files:\n",
    "    clip_dir = state_action_path.parent\n",
    "    clip_name = clip_dir.parent.name  # e.g., clip_00021\n",
    "\n",
    "    data = np.load(state_action_path, allow_pickle=True)\n",
    "    states = data[:, :6]\n",
    "    true_actions = data[:, 6:]\n",
    "    input_seq = []\n",
    "    predicted_actions = []\n",
    "\n",
    "    for i in range(len(states)):\n",
    "        input_seq.append(states[i])\n",
    "        if len(input_seq) < context_len:\n",
    "            continue\n",
    "        elif len(input_seq) > context_len:\n",
    "            input_seq.pop(0)\n",
    "\n",
    "        input_tensor = torch.tensor([np.array(input_seq)], dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            pred = model(input_tensor).squeeze().numpy()\n",
    "        predicted_actions.append(pred)\n",
    "\n",
    "    # Align ground truth to match predicted sequence length\n",
    "    gt_aligned = true_actions[context_len - 1 : context_len - 1 + len(predicted_actions)]\n",
    "\n",
    "    # Save to CSV\n",
    "    save_path = output_dir / f\"{clip_name}_pred_vs_gt.csv\"\n",
    "    with open(save_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Step\"] +\n",
    "                        [f\"GT_J{i+1}\" for i in range(6)] +\n",
    "                        [f\"Pred_J{i+1}\" for i in range(6)])\n",
    "        for step, (gt, pred) in enumerate(zip(gt_aligned, predicted_actions)):\n",
    "            writer.writerow([step] + list(np.round(gt, 3)) + list(np.round(pred, 3)))\n",
    "\n",
    "    print(f\"Saved: {save_path}\")\n",
    "\n",
    "print(\"All clips processed successfully. CSV files saved to: results/trajectories/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10011af-af16-4315-ab24-7f1168a18423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Compute MAE and max error for all clips and export summary as CSV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "input_dir = Path(\"results/trajectories\")\n",
    "output_csv = Path(\"results/trajectory_error_summary.csv\")\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "csv_files = sorted(input_dir.glob(\"clip_*_pred_vs_gt.csv\"))\n",
    "print(f\"Analyzing {len(csv_files)} trajectory comparison files...\")\n",
    "\n",
    "for file in csv_files:\n",
    "    clip_name = file.stem.replace(\"_pred_vs_gt\", \"\")\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    gt = df[[f\"GT_J{i+1}\" for i in range(6)]].values\n",
    "    pred = df[[f\"Pred_J{i+1}\" for i in range(6)]].values\n",
    "    error = np.abs(pred - gt)\n",
    "\n",
    "    mae = np.mean(error, axis=0)      # Mean Absolute Error per joint\n",
    "    maxe = np.max(error, axis=0)      # Maximum absolute error per joint\n",
    "    avg_mae = np.mean(mae)            # Overall mean MAE across all joints\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"Clip\": clip_name,\n",
    "        **{f\"MAE_J{i+1}\": round(mae[i], 3) for i in range(6)},\n",
    "        **{f\"MaxErr_J{i+1}\": round(maxe[i], 3) for i in range(6)},\n",
    "        \"Avg_MAE_AllJoints\": round(avg_mae, 3)\n",
    "    })\n",
    "\n",
    "# Create DataFrame and save to CSV\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "df_summary.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Trajectory error summary saved to: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5a851-32bc-4643-b1d7-d9dc178dc47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Visualize average MAE per clip using a bar chart\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load the previously generated error summary\n",
    "df = pd.read_csv(\"results/trajectory_error_summary.csv\")\n",
    "\n",
    "# Sort by clip name to ensure consistent bar order\n",
    "df = df.sort_values(\"Clip\")\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(14, 6))\n",
    "bars = plt.bar(df[\"Clip\"], df[\"Avg_MAE_AllJoints\"], color=\"skyblue\")\n",
    "\n",
    "# Set titles and labels\n",
    "plt.title(\"Average MAE per Clip (All Joints)\", fontsize=16)\n",
    "plt.xlabel(\"Clip\", fontsize=12)\n",
    "plt.ylabel(\"Average MAE (deg)\", fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Annotate each bar with its value\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.2,\n",
    "             f\"{yval:.2f}\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Save plot\n",
    "plot_path = \"results/mae_bar_chart_all_clips.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average MAE bar chart saved to: {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ec431-8547-4bd2-b4bd-032aee1d990f",
   "metadata": {},
   "source": [
    "### Section 7: Vision guided ACT model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f61827-e3ba-4949-93a2-7e1aa833b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set project root path to ensure proper module import\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "project_root = \"/home/wjw/lerobot\"\n",
    "sys.path.insert(0, project_root)\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(\"Current project path:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a962cacb-d853-4d12-9f31-b311fa130094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize robot and cameras (enhanced version)\n",
    "import os\n",
    "import time\n",
    "import threading\n",
    "import numpy as np\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from lerobot.common.robot_devices.robots.configs import KochRobotConfig\n",
    "from lerobot.common.robot_devices.robots.manipulator import ManipulatorRobot\n",
    "from lerobot.common.robot_devices.cameras.configs import OpenCVCameraConfig\n",
    "\n",
    "# Configure robot and attach cameras\n",
    "robot_cfg = KochRobotConfig()\n",
    "robot_cfg.cameras = {\n",
    "    \"e22s_side\": OpenCVCameraConfig(camera_index=0, fps=30, width=640, height=480),  # E22S side view\n",
    "    \"e12_top\": OpenCVCameraConfig(camera_index=4, fps=30, width=640, height=480),    # E12 top view\n",
    "}\n",
    "\n",
    "# Initialize robot instance\n",
    "robot = ManipulatorRobot(robot_cfg)\n",
    "\n",
    "# Attempt to connect to the robot\n",
    "print(\"Connecting to robot (initial calibration will run if needed)...\")\n",
    "try:\n",
    "    robot.connect()\n",
    "    print(\"Robot connection established.\")\n",
    "\n",
    "    # Display calibration directory path\n",
    "    print(\"Calibration directory:\", robot_cfg.calibration_dir)\n",
    "except Exception as e:\n",
    "    print(f\"Robot connection failed: {e}\")\n",
    "\n",
    "# Display camera device information\n",
    "print(\"Cameras successfully loaded:\")\n",
    "for name, cam in robot_cfg.cameras.items():\n",
    "    print(f\" - {name}: /dev/video{cam.camera_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3458f-0cff-48bc-8066-85aba2176e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly start synchronous control of the robot arm and turn on the camera\n",
    "! python lerobot/scripts/control_robot.py \\\n",
    "  --robot.type=koch \\\n",
    "  --control.type=teleoperate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15270104-3900-4fa0-adc3-4d05c9051a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Hugging face community\n",
    "! huggingface-cli login --token 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 --add-to-git-credential\n",
    "! HF_USER=$(huggingface-cli whoami | head -n 1)\n",
    "! echo $HF_USER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b01b7-2d6f-41e9-aa39-a0fd3d2987ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The final code for collecting data is best executed in the terminal\n",
    "\n",
    "! python lerobot/scripts/control_robot.py \\\n",
    "  --robot.type=koch \\\n",
    "  --control.type=record \\\n",
    "  --control.fps=60 \\\n",
    "  --control.single_task=\"Grasp with vision-state sync\" \\\n",
    "  --control.repo_id=JJwuj/koch_static_grasp_0402_v5 \\\n",
    "  --control.root=/home/wjw/lerobot/results_v5 \\\n",
    "  --control.tags='[\"koch\", \"static\", \"vision\"]' \\\n",
    "  --control.warmup_time_s=3 \\\n",
    "  --control.episode_time_s=20 \\\n",
    "  --control.reset_time_s=5 \\\n",
    "  --control.num_episodes=15 \\\n",
    "  --control.push_to_hub=true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b9f820-f54c-47c2-bc59-3ee58bd0cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay or verify data (optional) - Reproduce\n",
    "!python lerobot/scripts/control_robot.py \\\n",
    "  --robot.type=koch \\\n",
    "  --control.type=replay \\\n",
    "  --control.fps=60 \\\n",
    "  --control.repo_id=JJwuj/koch_static_grasp_0402_v5 \\\n",
    "  --control.episode=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4bd5e1-7c1a-4c79-8935-3b3be8a6bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training code\n",
    "\n",
    "! python lerobot/scripts/train.py \\\n",
    "  --dataset.repo_id=JJwuj/koch_static_grasp_0402_v5 \\\n",
    "  --policy.type=act \\\n",
    "  --wandb.enable=false \\\n",
    "  --output_dir=outputs/train/act_koch_real_v5 \\\n",
    "  --job_name=act_koch_real_v5 \\\n",
    "  --use_policy_training_preset=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46727c1e-06af-4f99-aa90-47bcc482622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the training model\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read training log from file\n",
    "with open(\"training_log.md\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "steps = []\n",
    "losses = []\n",
    "\n",
    "# Extract step and loss values using regular expressions\n",
    "for line in lines:\n",
    "    match = re.search(r\"step:(\\d+) .*? loss:(\\d+\\.\\d+)\", line)\n",
    "    if match:\n",
    "        steps.append(int(match.group(1)))\n",
    "        losses.append(float(match.group(2)))\n",
    "\n",
    "# Plot training loss curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(steps, losses, label=\"Training Loss\", color='red')\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Training Loss Curve for Vision-Guided ACT Model\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"vision_act_loss_curve.png\", dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3d6cd-8855-401e-b457-6a6ce986c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the training model\n",
    "\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load training log file\n",
    "log_text_path = \"/training_log.md\"  # Update path if needed\n",
    "with open(log_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "steps = []\n",
    "losses = []\n",
    "\n",
    "# Extract step and loss values (supporting \"K\" notation like 42K)\n",
    "for line in lines:\n",
    "    match = re.search(r\"step:(\\d+[K]?) .*? loss:(\\d+\\.\\d+)\", line)\n",
    "    if match:\n",
    "        step_str = match.group(1)\n",
    "        step = int(step_str.replace(\"K\", \"000\"))  # Convert \"42K\" â 42000\n",
    "        loss = float(match.group(2))\n",
    "        steps.append(step)\n",
    "        losses.append(loss)\n",
    "\n",
    "# Plot training loss curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(steps, losses, marker='o', markersize=3, linewidth=1.5, color='darkred', label=\"Training Loss\")\n",
    "plt.xlabel(\"Training Step\", fontsize=12)\n",
    "plt.ylabel(\"Loss (MSE)\", fontsize=12)\n",
    "plt.title(\"Training Loss Curve for Vision-Guided ACT Model\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "output_path = \"/mnt/data/vision_act_training_loss.png\"\n",
    "plt.savefig(output_path, dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09b6bd-65a1-4278-9133-ac2e923d272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the training model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# Load loss data\n",
    "df = pd.read_csv(\"step_loss.csv\")\n",
    "steps = df[\"step\"].values\n",
    "losses = df[\"loss\"].values\n",
    "\n",
    "# 1. Estimate early-stage convergence rate (linear fit before 5K steps)\n",
    "early_mask = steps <= 5000\n",
    "early_steps = steps[early_mask]\n",
    "early_losses = losses[early_mask]\n",
    "early_slope = np.polyfit(early_steps, early_losses, deg=1)[0]\n",
    "print(f\"Early-stage loss decrease rate: {early_slope:.4f} per step\")\n",
    "\n",
    "# 2. Compute standard deviation in stable convergence zone (30Kâ50K steps)\n",
    "stable_mask = (steps >= 30000) & (steps <= 50000)\n",
    "stable_loss_std = np.std(losses[stable_mask])\n",
    "print(f\"Loss standard deviation in stable zone (30Kâ50K): {stable_loss_std:.5f}\")\n",
    "\n",
    "# 3. Exponential curve fitting\n",
    "def exp_func(x, a, b, c):\n",
    "    return a * np.exp(-b * x) + c\n",
    "\n",
    "scaled_steps = steps / 1000  # Normalize step values to improve fitting stability\n",
    "popt, _ = curve_fit(exp_func, scaled_steps, losses, p0=(5, 1, 0.03))\n",
    "fitted_losses = exp_func(scaled_steps, *popt)\n",
    "\n",
    "# 4. Plot original loss curve and exponential fit\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, losses, label=\"Observed Loss\", color='red')\n",
    "plt.plot(steps, fitted_losses, label=\"Exponential Fit\", linestyle='--', color='blue')\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve with Exponential Fit\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/loss_exp_fit.png\", dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855a65d-d6f5-4126-b6ae-3db4d63b2271",
   "metadata": {},
   "source": [
    "### Fianl Deployment of Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e27313-1fff-43fc-8cab-d3e28ce581a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ï¼python lerobot/scripts/control_robot.py \\\n",
    "  --robot.type=koch \\\n",
    "  --control.type=record \\\n",
    "  --control.fps=30 \\\n",
    "  --control.single_task=\"Grasp with vision-state sync\" \\\n",
    "  --control.repo_id=JJwuj/eval_koch_static_grasp_0403f \\\n",
    "  --control.root=/home/wjw/lerobot/results_train5 \\\n",
    "  --control.tags='[\"koch\", \"eval\"]' \\\n",
    "  --control.warmup_time_s=5 \\\n",
    "  --control.episode_time_s=40 \\\n",
    "  --control.reset_time_s=10 \\\n",
    "  --control.num_episodes=3 \\\n",
    "  --control.push_to_hub=false \\\n",
    "  --control.policy.path=outputs/train/act_koch_real_v5/checkpoints/last/pretrained_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ea3ef5-32bd-49e6-9b70-5bc12cbdcb90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lerobot)",
   "language": "python",
   "name": "lerobot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
